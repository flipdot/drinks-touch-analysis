#!/usr/bin/env python3
# Loads drinks purchase statistics with according user database and plots it in a meaningful way.

import argparse
import datetime
import logging
import os
import psycopg2
import requests
import sys

import pandas as pd

from pathlib import Path
from bs4 import BeautifulSoup # TODO Fix version in requirements.txt
from sshtunnel import SSHTunnelForwarder, HandlerSSHTunnelForwarderError

LDAP_SCRAPE_HOST = '192.168.3.231' # power-pi.fd ?
DRINKS_DATA_HOST = '192.168.3.231' # drinks.fd

SSH_PORT = 22
SSH_USERNAME = '' # Please see:
SSH_PASSWORD = '' # https://www.raspberrypi.org/documentation/linux/usage/users.md
SSH_REMOTE_BIND = ('127.0.0.1', 5432)
SSH_LOCAL_BIND = SSH_REMOTE_BIND

DATA_DIRECTORY = 'data'
CSV_SEPARATOR = ';'
LDAP_DATA_FILE = '{}/users.csv'.format(DATA_DIRECTORY)
DRINKS_DATA_FILE = '{}/drinks.csv'.format(DATA_DIRECTORY)
SCAN_DATA_FILE = '{}/scans.csv'.format(DATA_DIRECTORY)
RECHARGE_DATA_FILE = '{}/recharges.csv'.format(DATA_DIRECTORY)

LOG_DIRECTORY = 'logs'
DATE_STR = datetime.date.today().isoformat()
FILE_STR = os.path.basename(__file__).replace('.py', '')
LOG_FILE = os.path.join(LOG_DIRECTORY, "{}-{}.log".format(FILE_STR, DATE_STR))

LOG_LEVELS = {
    'debug': logging.DEBUG,
    'info': logging.INFO,
    'warning': logging.WARNING,
    'error': logging.ERROR,
    'critical': logging.CRITICAL,
}


def get_users(reload=False):
    # Load local data if it exists and reloading is disabled
    ldap_data_file = Path(LDAP_DATA_FILE)
    if not reload and ldap_data_file.is_file():
        return pd.read_csv(LDAP_DATA_FILE, sep=CSV_SEPARATOR, dtype={
            'USER_NAME': str,
            'USER_ID': str
        })

    # Request scraping content
    r = requests.get('http://{}/'.format(LDAP_SCRAPE_HOST))
    if r.status_code != 200:
        raise ConnectionError('Could not connect to LDAP scrape host ({})!'.format(LDAP_SCRAPE_HOST))
    if r.text == '':
        raise ConnectionError('Could not collect data from LDAP scrape host ({})!'.format(LDAP_SCRAPE_HOST))

    # Scrape
    soup = BeautifulSoup(r.text, 'html.parser')
    users = ['USER_NAME']
    uids = ['USER_ID']
    for option in soup.find_all('option'):
        user = option.get_text().strip()
        uid = option.get('value').strip()
        # Skip erroneous/empty entries
        if not user or not uid or user == '' or uid == '':
            continue
        users.append(user)
        uids.append(uid)

    # Create data directory if necessary
    if not os.path.exists(DATA_DIRECTORY):
        os.makedirs(DATA_DIRECTORY)

    csv_data = pd.DataFrame({users[0]: users[1:], uids[0]: uids[1:]}).drop_duplicates()

    # Save data to disk
    csv_data.to_csv(LDAP_DATA_FILE, sep=CSV_SEPARATOR, index=False)
    logger.info("Saved data to: {}/{}".format(os.getcwd(), LDAP_DATA_FILE))
    return get_users()


def postgres_query(query):
    try:
        conn = psycopg2.connect("dbname='drinks' user='postgres' host='localhost'")
    except ConnectionError as e:
        raise ConnectionError("Unable to connect to the database:\n{}".format(e))

    cur = conn.cursor()
    cur.execute(query)
    rows = cur.fetchall()
    return rows


def ssh_postgres_query(query):
    try:
        with SSHTunnelForwarder(
                (DRINKS_DATA_HOST, SSH_PORT),
                ssh_username=SSH_USERNAME,
                ssh_password=SSH_PASSWORD,
                remote_bind_address=SSH_REMOTE_BIND,
                local_bind_address=SSH_LOCAL_BIND,
        ):
            return postgres_query(query)
    except ValueError as e:
        logger.error("Error with SSH credentials: {}".format(e))
        raise e
    except HandlerSSHTunnelForwarderError as e:
        logger.error("Could not rebind local SSH {}:{}. Is it already bound by another process?".format(SSH_LOCAL_BIND[0], SSH_LOCAL_BIND[1]))
        raise e


def postgres_data(map_dict, sql_query, data_file_path, reload=False):
    # Load local data if it exists and reloading is disabled
    data_file = Path(data_file_path)
    if not reload and data_file.is_file():
        map_dtypes = {}
        for k, v in map_dict.items():
            map_dtypes[k] = v[1]
        return pd.read_csv(data_file_path, sep=CSV_SEPARATOR, dtype=map_dtypes)

    # Query data otherwise
    try:
        data_raw = ssh_postgres_query(sql_query)
    except ValueError:
        sys.exit(10)
    except HandlerSSHTunnelForwarderError:
        sys.exit(11)
    data_raw = pd.DataFrame(data_raw)
    data = {}
    for k, v in map_dict.items():
        data[k] = data_raw[v[0]]
    data = pd.DataFrame(data)

    # Save data to disk
    data.to_csv(data_file_path, sep=CSV_SEPARATOR, index=False)
    logger.info("Saved data to: {}/{}".format(os.getcwd(), data_file_path))
    return postgres_data(map_dict, sql_query, data_file_path, reload=False)


def get_drinks(reload=False):
    map_dict = {
        "EAN": (1, str),
        "NAME": (2, str),
        "SIZE": (3, float),
        "TYPE": (5, str),
    }
    sql_query = "SELECT * FROM drink;"
    data_file_path = DRINKS_DATA_FILE
    return postgres_data(map_dict, sql_query, data_file_path, reload=reload)


def get_scans(reload=False):
    map_dict = {
        "EAN": (1, str),
        "USER_ID": (2, str),
        "TIMESTAMP": (3, str),
    }
    sql_query = "SELECT * FROM scanevent;"
    data_file_path = SCAN_DATA_FILE
    data = postgres_data(map_dict, sql_query, data_file_path, reload=reload)
    data['TIMESTAMP'] = pd.to_datetime(data['TIMESTAMP'])
    return data


def get_recharges(reload=False):
    map_dict = {
        "USER_ID": (1, str),
        "HELPER_USER_ID": (2, str),
        "AMOUNT": (3, float),
        "TIMESTAMP": (4, str),
    }
    sql_query = "SELECT * FROM rechargeevent;"
    data_file_path = RECHARGE_DATA_FILE
    data = postgres_data(map_dict, sql_query, data_file_path, reload=reload)
    data['TIMESTAMP'] = pd.to_datetime(data['TIMESTAMP'])
    return data



if __name__ == '__main__':
    # Create empty directories if they don't exist
    for dir in (LOG_DIRECTORY, DATA_DIRECTORY):
        if not os.path.exists(dir):
            os.makedirs(dir)

    # Argument parsing
    parser = argparse.ArgumentParser()
    parser.add_argument('--disable-stdout', action='store_true',
                        help='Disable output to stdout')
    parser.add_argument('--disable-logging', action='store_true',
                        help='Disable logging')
    parser.add_argument('--reload-data', action='store_true',
                        help='Enable reloading locally cached data')
    parser.add_argument('--log-level', choices=LOG_LEVELS,
                        help='Set log level accordingly')
    args = parser.parse_args()

    # Create logger
    logger = logging.getLogger()
    formatter = logging.Formatter('%(asctime)s %(levelname)-8s %(message)s')
    if not args.log_level:
        logger.setLevel(logging.INFO)
    else:
        logger.setLevel(LOG_LEVELS[args.log_level])

    if not args.disable_stdout:
        handler_stream = logging.StreamHandler()
        handler_stream.setFormatter(formatter)
        logger.addHandler(handler_stream)

    if not args.disable_logging:
        handler_file = logging.FileHandler(LOG_FILE)
        handler_file.setFormatter(formatter)
        logger.addHandler(handler_file)

    users = get_users(reload=args.reload_data)
    drinks = get_drinks(reload=args.reload_data)

    scans = get_scans(reload=args.reload_data)
    scans_full = drinks.merge(users.merge(scans, 'inner', 'USER_ID'), 'inner', 'EAN')

    recharges = get_recharges(reload=args.reload_data)
    recharges_full = users.merge(users.merge(recharges, 'inner', 'USER_ID'), 'inner', left_on='USER_ID', right_on='HELPER_USER_ID')
    recharges_full = recharges_full.drop(columns=('HELPER_USER_ID'))
    recharges_full = recharges_full.rename(columns={
        'USER_NAME_x' : 'HELPER_USER_NAME',
        'USER_ID_x' : 'HELPER_USER_ID',
        'USER_NAME_y' : 'USER_NAME',
        'USER_ID_y' : 'USER_ID',
    })

    # TODO Interpret data
